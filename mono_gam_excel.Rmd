---
title: "Shape Constrained Regression for Excel"
author: "William Chiu"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Summary

Generalized Additive Models (gam) estimate non-linear but additive relationships between a response and a set of predictors. However, when the training data has a high noise to signal ratio, gam could estimate wiggly (non-monotonic) relationships that are difficult to explain. Shape Constrained Additive Models (scam) impose user-defined monotonic relationships that can be explained by the user.

The `mgcv` and `scam` packages are highly effective tools for developing gam and scam models. Both packages use the p-spline basis to construct non-linear features. Although p-splines have desirable properties, they are difficult to implement in other software tools like Excel. There is no closed-form equation that I could export from gam/scam and into Excel for an end-user.

Multivariate Adaptive Regression Splines (mars), from the `earth` package, allow us to approximate the scam model with linear basis functions -- which are Excel-friendly. 

Through a data simulation, I demonstrate how to fit gam and scam models using the `mgcv` and `scam` packages. Then I approximate the scam model and provide an Excel-friendly regression equation. Reader should be aware that MARS does *not guarantee* monotonicity. Therefore, after fitting an approximation of the scam model, reader should test the predictions by feeding extreme values of the predictors into the final model.

We also explore the `scar` package which uses monotonic binning. Then I approximate the scar model with non-linear least squares and sigmoid functions. Sigmoid functions guarantee monotonicity. However, sigmoid functions assume that the response tapers off on the tails (i.e., the relationship is S-shaped).

Approximations are achieved by turning the logistic regression problem into a least squares problem. The trick is to use a pseudo response that is continuous (rather than binary).

# Data Generating Process (DGP)

The chunk generates 3 data frames: full data, training data, and test data. To demonstrate the wiggliness associated with gam, the sample size of the training data is limited to only 2000 observations.

Each data frame contains a binary response and two predictors. The first predictor has a monotonically increasing relationship with the response, while the second predictor has a monotonically decreasing relationship with the response. Both relationships are sigmoidal and hence highly non-linear.

In addition, the event rate of the response is rare.

```{r, message=FALSE}
library(mgcv)
library(scam)
library(tidyverse)
library(caTools)
library(MLmetrics)
library(earth)
library(plotmo)
library(furrr)
library(scar)
library(minpack.lm)
library(car)

plan(multisession, workers = 6) # set to 2 for most PCs

set.seed(2001)

nobs <- 100000

x1 <- rnorm(nobs)
x2 <- rnorm(nobs)

# Population log odds is a function of sigmoid features

z <- -35 + 5 * SSfpl(x1, -2, 2, 0, 0.5) - 5 * SSfpl(x2, -1, 1, 0, 0.1) +
  rnorm(nobs,0,20)

y <- rbinom(nobs, 1, prob=boot::inv.logit(z))

full_data <- data.frame(y=y, x1=x1, x2=x2)

summary(full_data)

train_index <- sample.split(full_data$y, SplitRatio = 2/100)

train_data <- full_data[train_index,] # small
test_data <- full_data[!train_index,] # large

```

# GAM

Due to the high noise to signal ratio in the training data, gam estimates very wiggly relationships between the binary response and the predictors. The y-axis is on the log-odds (or logit) scale.

```{r}
## fit a gam

## when population error is high, gam returns non-monotonic relationships
## even when the population relationships are monotonic

mod_gam <- gam(y ~ s(x1) + s(x2), data = train_data, family = binomial)

summary(mod_gam)

plot(mod_gam, pages=1, se=FALSE, main="Unconstrained GAM",
     shift=coef(mod_gam)[1])

```

# SCAM

Shape constraints on each predictor reduce variance (wiggliness). Out-of-sample performance is similar between `gam` and `scam`.

```{r}
## gam with monotonic constraints

## scam imposes monotonic relationships between each
## feature and the binary response
## mpi = monotonic p-spline increasing
## mpd = monotonic p-spline decreasing

mod_mono_gam <- scam(y ~ s(x1, bs="mpi") + s(x2, bs="mpd"),
                     data = train_data, family = binomial)

summary(mod_mono_gam)

plot(mod_mono_gam, pages=1, se=FALSE, main="Constrained GAM",
     shift=coef(mod_mono_gam)[1])

```

```{r}
## helper function for test set performance

perf <- function(lst_preds, f_metric=caTools::colAUC, metricname="ROC-AUC"){
  map_dfr(lst_preds, function(x){
  f_metric(x, test_data$y)
}) %>% 
  pivot_longer(everything(), names_to="model", values_to=metricname) %>% 
  knitr::kable()
}
```

```{r}

map_dfr(list(mod_gam=mod_gam, mod_mono_gam=mod_mono_gam), AIC) %>% 
  pivot_longer(everything(), names_to="model", values_to="AIC") %>% 
  knitr::kable()


## test set performance

preds_gam <- predict(mod_gam, newdata=test_data, type="response")
preds_mono_gam <- predict(mod_mono_gam, newdata=test_data,
                          type="response")

myPreds <- list(gam=preds_gam, mono_gam=preds_mono_gam)

perf(myPreds, caTools::colAUC, "ROC-AUC")

perf(myPreds, MLmetrics::LogLoss, "LogLoss")

```



# Generate Pseudo-Training Data

The next step is controversial. I pretend that the scam model is the data generating process. In the actual training data, we observe binary outcomes rather than the log odds. In the pseudo data, I ignore the binary outcomes and "observe" the log odds from the scam model.

The reader should be aware that in the actual training data, `y` is a binary response. In the pseudo data, `y` is log odds from the scam model, which is analogous to `z` in the true data generating process.

```{r}
## generate pseudo-data using Mono GAM

## suppose the scam model is the data generating process (dgp).
## append the log odds for each observation in the training
## data

pseudo_train_data <- train_data 

pseudo_train_data$y <- as.numeric(predict(mod_mono_gam, newdata=train_data))

summary(pseudo_train_data)


```

# Approximate SCAM with MARS

```{r}
## approximate the Mono GAM model with MARS

## unfortunately, both gam and scam models are very difficult to 
## implement outside of R due to how the basis functions are defined.
## an alternative is to approximate the scam relationships
## with simpler basis functions like the linear basis (which can be 
## easily implemented in Excel) -- the linear basis is also called the
## reLU (rectified linear unit)


mod_logit_pseudo <- earth(y ~ x1 + x2, data=pseudo_train_data)

summary(mod_logit_pseudo)

plotmo(mod_logit_pseudo, caption="MARS with Pseudo Data")


```

Since the pseudo model is in the log-odds scale, I need to convert the predictions into probabilities to compare against the probability predictions from gam and scam.

```{r}
## test set performance

preds_pseudo <- boot::inv.logit(predict(mod_logit_pseudo, newdata=test_data))

myPreds <- list(gam=preds_gam, mono_gam=preds_mono_gam, pseudo_mono=preds_pseudo)

perf(myPreds, caTools::colAUC, "ROC-AUC")

perf(myPreds, MLmetrics::LogLoss, "LogLoss")
```


# Excel-friendly Equation

The equation below predicts the log odds or logit.

```{r}
cat(format(mod_logit_pseudo, style="pmax", use.names=TRUE))


```

```{r}
## Compare predict() against score_function

as.func <- function(object, digits = 20, use.names = TRUE, ...)
  eval(parse(text=paste(
    "function(x){\n",
    "if(is.vector(x))\n",
    "  x <- matrix(x, nrow = 1, ncol = length(x))\n",
    "with(as.data.frame(x),\n",
    format(object, digits = digits, use.names = use.names, style = "pmax", ...),
    ")\n",
    "}\n", sep = "")))

score_function <- as.func(mod_logit_pseudo)

compare_df <- expand.grid(x1 = seq(-10, 10, 0.1),
           x2 = seq(-10, 10, 0.1))

earth_preds <- predict(mod_logit_pseudo, newdata=compare_df)

score_preds <- score_function(compare_df)

max(abs(earth_preds - score_preds))
```

# Testing monotonicity

MARS does not guarantee monotonic relationships. Reader should always test the model for violations of monotonicity by feeding extreme predictor values into the model.

```{r}
mono_check <- function(x, increasing=TRUE){
  if(increasing==TRUE){
    out <- all(x == cummax(x))
  } else {
    out <- all(x==cummin(x))
  }
  
  return(out)
}

df_x1 <- data.frame(x1 = seq(-10,10, 0.1),
                    x2 = mean(train_data$x2))

df_x2 <- data.frame(x1 = mean(train_data$x1),
                    x2 = seq(-10,10, 0.1))

df_x1$log_odds <- score_function(df_x1)

df_x2$log_odds <- score_function(df_x2)

mono_check(df_x1$log_odds)

mono_check(df_x2$log_odds, FALSE)

ggplot(df_x1, aes(x=x1, y=log_odds)) + geom_line() + theme_bw()

ggplot(df_x2, aes(x=x2, y=log_odds)) + geom_line() + theme_bw()
```

# How often does monotonicity fail?

Since MARS does not guarantee monotonicity. I repeated, 60 times, the following steps:

1. Set a new seed value
2. Generate full, train, and test sets
3. Fit a scam model to the training data
4. Generate pseudo data
5. Fit a MARS model to the pseudo data
6. Check shape constraints (x1 should increase with y and x2 should decrease with y)

```{r, warning=FALSE}
simulate_one <- function(myseed){
  
  # Set a new seed value
    
  set.seed(myseed)
  
  nobs <- 100000
  
  x1 <- rnorm(nobs)
  x2 <- rnorm(nobs)
  
  z <- -35 + 5 * SSfpl(x1, -2, 2, 0, 0.5) - 5 * SSfpl(x2, -1, 1, 0, 0.1) +
    rnorm(nobs,0,20)
  
  y <- rbinom(nobs, 1, prob=boot::inv.logit(z))
  
  # Generate full, train, and test sets
  full_data <- data.frame(y=y, x1=x1, x2=x2)
  
  train_index <- sample.split(full_data$y, SplitRatio = 2/100)
  train_data <- full_data[train_index,] # small
  test_data <- full_data[!train_index,] # large
  
  # SCAM
  
  my_mod_mono_gam <- scam(y ~ s(x1, bs="mpi") + s(x2, bs="mpd"),
                       data = train_data, family = binomial)
  
  # Pseudo data
  
  pseudo_train_data <- train_data 
  
  pseudo_train_data$y <- as.numeric(predict(my_mod_mono_gam, newdata=train_data))
  
  summary(pseudo_train_data)
  
  # MARS
  
  my_mod_pseudo_mars <- earth(y ~ x1 + x2, data=pseudo_train_data)
  
  
  # Check shape constraints
  
  df_x1 <- data.frame(x1 = seq(-10,10, 0.1),
                      x2 = mean(train_data$x2))
  
  df_x2 <- data.frame(x1 = mean(train_data$x1),
                      x2 = seq(-10,10, 0.1))
  
  df_x1$log_odds <- predict(my_mod_pseudo_mars, newdata=df_x1)
  
  df_x2$log_odds <- predict(my_mod_pseudo_mars, newdata=df_x2)
  
  success <- mono_check(df_x1$log_odds) & mono_check(df_x2$log_odds, FALSE)
  
  return(success)
}

mono_success <- future_map_int(1:60, simulate_one,
                         .options=furrr_options(seed=NULL,
                                                packages=c('earth', 'scam')
                                                )
                         )

future:::ClusterRegistry("stop")


mean(mono_success)
```

MARS successfully approximated the scam model about `r mean(mono_success)*100`% of the time.


# Why Pseudo Data?

The pseudo data filters out the noise in the latent response (`z`), which drives the binary response (`y`). Although we could directly pass the actual training data into MARS, the noise in the data generating process could cause `earth` to return non-monotonic relationships.

Reader should notice that `y` in the actual training data is a binary outcome.

```{r}
mod_earth <- earth(y ~ x1 + x2, data=train_data, glm=list(family=binomial))

summary(mod_earth)

plotmo(mod_earth, type="link", caption="MARS with Training Data")
```

# Alternative to SCAM: SCAR

Models developed in `scam` return smooth relationships between a response and each predictor. However, the smoothness (and monotonicity) is achieved through the p-spline basis, which is difficult to implement. If smoothness is not required, an alternative approach is to use monotonic step functions to approximate the relationships. The `scar` package achieves this goal.

Unfortunately, the `scar` package does not take data frames. I wrote two helper functions to convert a data frame into a matrix, which is then passed to the scar function.

```{r}
fit_scar <- function(formula, shape=rep("l", d), data, family=gaussian(),
                     weights=rep(1, length(y)), epsilon = 1e-08){
  
  x <- model.matrix(formula, data)[,-1]
  
  y <- data[,all.vars(formula)[1]]
  
  mod <- scar(x, y, shape, family, weights, epsilon)
  
  mod$formula <- formula
  
  return(mod)
}

predict_scar <- function(object, newdata, type = c("link", "response"),
                         rule=1, ...){
  
  fmla <- object$formula
  
  pred_names <- all.vars(fmla)[-1]
  
  x <- as.matrix(newdata[,pred_names])
  
  preds <- predict(object, x, type, rule, ...)
  
  return(preds)
}
```

Now we can fit a scar model with monotonic binning.

```{r}
mod_scar <- fit_scar(y ~ x1 + x2, shape=c("in", "de"), 
                     data=train_data, family=binomial())
```

Inside `mod_scar` is an object called `componentfit`. It contains partial fitted values by predictor (column) and observation (row). Each observation in the training set corresponds to a row in `componentfit`. The fitted value for a single observation is the row sum of the partial fitted values.

Given a pair of new predictor values (that are not in the training data), the `predict` function interpolates the partial fitted values from `componentfit`. This may be problematic if `componentfit` is very big. A similar problem arises with generating predictions from k-nearest-neighbors (knn).

The interpolation of partial fitted values causes the relationship between a response and a predictor to appear "step-like".

Check the partial plots.

```{r}
df_x1$log_odds <- predict_scar(mod_scar, newdata=df_x1)
  
df_x2$log_odds <- predict_scar(mod_scar, newdata=df_x2)

ggplot(df_x1, aes(x=x1, y=log_odds)) + geom_line() + theme_bw()
ggplot(df_x2, aes(x=x2, y=log_odds)) + geom_line() + theme_bw()
```

# Check test set performance

```{r}
preds_scar <- predict_scar(mod_scar, newdata=test_data, type="response")

myPreds <- list(gam=preds_gam, mono_gam=preds_mono_gam, pseudo_mono=preds_pseudo,
             scar=preds_scar)

perf(myPreds, caTools::colAUC, "ROC-AUC")

perf(myPreds, MLmetrics::LogLoss, "Log Loss")
```

# Approximating SCAR

Similar to the MARS approximation of scam, we could approximate scar with another method (with the help of pseudo data). The pseudo response would be the log-odds from the scar model. The features would be sigmoid functions that are fed into non-linear least squares. Sigmoid functions guarantee monotonicity.

First, some pseudo data!

```{r}
pseudo_train_data_nls <- train_data 

pseudo_train_data_nls$y <- predict_scar(mod_scar, newdata=train_data
                                        , type="link")

summary(pseudo_train_data_nls)

```
Second, non-linear least squares and sigmoid features. The classical `nls` function is very sensitive to initial parameter guesses. I have never been successful in using it. I suggest using `nlsLM` from the `minpack.lm` package.

```{r}
mod_pseudo_scar <- nlsLM(y ~ constant + a * SSfpl(x1, A1, B1, xmid1, scal1) +
                           b * SSfpl(x2, A2, B2, xmid2, scal2),
                         data=pseudo_train_data_nls,
                         start=list(constant=0, a=1, b=1,
                                    A1=-1, B1=1, xmid1=0.5, scal1=0.5,
                                    A2=-1, B2=1, xmid2=0.5, scal2=0.5))


```

```{r}
est <- coef(mod_pseudo_scar)

est_df <- data.frame(Estimate=est)

knitr::kable(est_df)
```

We could also bootstrap 95% confidence intervals for each estimate.

```{r}
boot_psuedo <- Boot(mod_pseudo_scar, R=1000)

```

```{r}

CI_nls <- data.frame(confint(boot_psuedo, type="perc"))

names(CI_nls) <- c("p2.5", "p97.5")

CI_nls$signif <- (CI_nls$p2.5 * CI_nls$p97.5) > 0

knitr::kable(CI_nls)
```


Some plots.

```{r}
df_x1$log_odds <- predict(mod_pseudo_scar, newdata=df_x1)
  
df_x2$log_odds <- predict(mod_pseudo_scar, newdata=df_x2)

ggplot(df_x1, aes(x=x1, y=log_odds)) + geom_line() + theme_bw()
ggplot(df_x2, aes(x=x2, y=log_odds)) + geom_line() + theme_bw()

```

Test set performance

```{r}
preds_pseudo_scar <- boot::inv.logit(predict(mod_pseudo_scar, newdata=test_data))

myPreds <- list(gam=preds_gam, mono_gam=preds_mono_gam, pseudo_mono=preds_pseudo,
             scar=preds_scar, pseudo_scar=preds_pseudo_scar)

perf(myPreds, caTools::colAUC, "ROC-AUC")

perf(myPreds, MLmetrics::LogLoss, "Log Loss")
```

The approximation of scar actually outperforms the original scar model because the approximation extrapolates using the sigmoid functions that were estimated by non-linear least squares.

Sigmoid functions guarantee monotonicity.

# Conclusion

Imposing shape constraints on data with high noise to signal ratios could greatly reduce variance (wiggliness). The `scam` package is highly effective in developing models with user-defined monotonicity constraints for each predictor. MARS can approximate the scam model with linear basis functions, which are easy to write down on paper and implement in an Excel formula.

The `scar` package also develops models with user-defined monotonicity constraints for each predictor. Non-linear least squares can approximate the scar model with sigmoid functions, which are easy to write down on paper (`?SSfpl` for equation) and implement in an Excel formula.
